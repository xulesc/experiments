{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "### Models\n",
    "import requests\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaModel:\n",
    "    def __init__(self, model, system_prompt, temperature=0, stop=None):\n",
    "        \"\"\"\n",
    "        Initializes the OllamaModel with the given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        model (str): The name of the model to use.\n",
    "        system_prompt (str): The system prompt to use.\n",
    "        temperature (float): The temperature setting for the model.\n",
    "        stop (str): The stop token for the model.\n",
    "        \"\"\"\n",
    "        self.model_endpoint = \"http://localhost:11434/api/generate\"\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "        self.stop = stop\n",
    "\n",
    "    def generate_text(self, prompt):\n",
    "        \"\"\"\n",
    "        Generates a response from the Ollama model based on the provided prompt.\n",
    "\n",
    "        Parameters:\n",
    "        prompt (str): The user query to generate a response for.\n",
    "\n",
    "        Returns:\n",
    "        dict: The response from the model as a dictionary.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"format\": \"json\",\n",
    "            \"prompt\": prompt,\n",
    "            \"system\": self.system_prompt,\n",
    "            \"stream\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"stop\": self.stop\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            request_response = requests.post(\n",
    "                self.model_endpoint, \n",
    "                headers=self.headers, \n",
    "                data=json.dumps(payload)\n",
    "            )\n",
    "\n",
    "            print(\"REQUEST RESPONSE\", request_response)\n",
    "            request_response_json = request_response.json()\n",
    "            response = request_response_json['response']\n",
    "            response_dict = json.loads(response)\n",
    "\n",
    "            print(f\"\\n\\nResponse from Ollama model: {response_dict}\")\n",
    "\n",
    "            return response_dict\n",
    "        except requests.RequestException as e:\n",
    "            response = {\"error\": f\"Error in invoking model! {str(e)}\"}\n",
    "            return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_calculator(input_str):\n",
    "    \"\"\"\n",
    "    Perform a numeric operation on two numbers based on the input string.\n",
    "\n",
    "    Parameters:\n",
    "    input_str (str): A JSON string representing a dictionary with keys 'num1', 'num2', and 'operation'. Example: '{\"num1\": 5, \"num2\": 3, \"operation\": \"add\"}' or \"{'num1': 67869, 'num2': 9030393, 'operation': 'divide'}\"\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted result of the operation.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If an error occurs during the operation (e.g., division by zero).\n",
    "    ValueError: If an unsupported operation is requested or input is invalid.\n",
    "    \"\"\"\n",
    "    # Clean and parse the input string\n",
    "    try:\n",
    "        # Replace single quotes with double quotes\n",
    "        input_str_clean = input_str.replace(\"'\", \"\\\"\")\n",
    "        # Remove any extraneous characters such as trailing quotes\n",
    "        input_str_clean = input_str_clean.strip().strip(\"\\\"\")\n",
    "\n",
    "        input_dict = json.loads(input_str_clean)\n",
    "        num1 = input_dict['num1']\n",
    "        num2 = input_dict['num2']\n",
    "        operation = input_dict['operation']\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        return str(e), \"Invalid input format. Please provide a valid JSON string.\"\n",
    "\n",
    "    # Define the supported operations\n",
    "    operations = {\n",
    "        'add': operator.add,\n",
    "        'subtract': operator.sub,\n",
    "        'multiply': operator.mul,\n",
    "        'divide': operator.truediv,\n",
    "        'floor_divide': operator.floordiv,\n",
    "        'modulus': operator.mod,\n",
    "        'power': operator.pow,\n",
    "        'lt': operator.lt,\n",
    "        'le': operator.le,\n",
    "        'eq': operator.eq,\n",
    "        'ne': operator.ne,\n",
    "        'ge': operator.ge,\n",
    "        'gt': operator.gt\n",
    "    }\n",
    "\n",
    "    # Check if the operation is supported\n",
    "    if operation in operations:\n",
    "        try:\n",
    "            # Perform the operation\n",
    "            result = operations[operation](num1, num2)\n",
    "            result_formatted = f\"\\n\\nThe answer is: {result}.\\nCalculated with basic_calculator.\"\n",
    "            return result_formatted\n",
    "        except Exception as e:\n",
    "            return str(e), \"\\n\\nError during operation execution.\"\n",
    "    else:\n",
    "        return \"\\n\\nUnsupported operation. Please provide a valid operation.\"\n",
    "\n",
    "def reverse_string(input_string):\n",
    "    \"\"\"\n",
    "    Reverse the given string.\n",
    "\n",
    "    Parameters:\n",
    "    input_string (str): The string to be reversed.\n",
    "\n",
    "    Returns:\n",
    "    str: The reversed string.\n",
    "    \"\"\"\n",
    "    # Reverse the string using slicing\n",
    "    reversed_string = input_string[::-1]\n",
    "\n",
    "    reversed_string = f\"The reversed string is: {reversed_string}\\n\\n.Executed using the reverse_string function.\"\n",
    "    # print (f\"DEBUG: reversed_string: {reversed_string}\")\n",
    "    return reversed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolBox:\n",
    "    def __init__(self):\n",
    "        self.tools_dict = {}\n",
    "\n",
    "    def store(self, functions_list):\n",
    "        \"\"\"\n",
    "        Stores the literal name and docstring of each function in the list.\n",
    "\n",
    "        Parameters:\n",
    "        functions_list (list): List of function objects to store.\n",
    "\n",
    "        Returns:\n",
    "        dict: Dictionary with function names as keys and their docstrings as values.\n",
    "        \"\"\"\n",
    "        for func in functions_list:\n",
    "            self.tools_dict[func.__name__] = func.__doc__\n",
    "        return self.tools_dict\n",
    "\n",
    "    def tools(self):\n",
    "        \"\"\"\n",
    "        Returns the dictionary created in store as a text string.\n",
    "\n",
    "        Returns:\n",
    "        str: Dictionary of stored functions and their docstrings as a text string.\n",
    "        \"\"\"\n",
    "        tools_str = \"\"\n",
    "        for name, doc in self.tools_dict.items():\n",
    "            tools_str += f\"{name}: \\\"{doc}\\\"\\n\"\n",
    "        return tools_str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_system_prompt_template = \"\"\"\n",
    "You are an agent with access to a toolbox. Given a user query, \n",
    "you will determine which tool, if any, is best suited to answer the query. \n",
    "You will generate the following JSON response:\n",
    "\n",
    "\"tool_choice\": \"name_of_the_tool\",\n",
    "\"tool_input\": \"inputs_to_the_tool\"\n",
    "\n",
    "tool_choice: The name of the tool you want to use. It must be a tool from your toolbox or \"no tool\" if you do not need to use a tool.\n",
    "tool_input: The specific inputs required for the selected tool. If no tool, just provide a response to the query.\n",
    "Here is a list of your tools along with their descriptions:\n",
    "{tool_descriptions}\n",
    "\n",
    "Please make a decision based on the provided user query and the available tools.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, tools, model_service, model_name, stop=None):\n",
    "        \"\"\"\n",
    "        Initializes the agent with a list of tools and a model.\n",
    "\n",
    "        Parameters:\n",
    "        tools (list): List of tool functions.\n",
    "        model_service (class): The model service class with a generate_text method.\n",
    "        model_name (str): The name of the model to use.\n",
    "        \"\"\"\n",
    "        self.tools = tools\n",
    "        self.model_service = model_service\n",
    "        self.model_name = model_name\n",
    "        self.stop = stop\n",
    "\n",
    "    def prepare_tools(self):\n",
    "        \"\"\"\n",
    "        Stores the tools in the toolbox and returns their descriptions.\n",
    "\n",
    "        Returns:\n",
    "        str: Descriptions of the tools stored in the toolbox.\n",
    "        \"\"\"\n",
    "        toolbox = ToolBox()\n",
    "        toolbox.store(self.tools)\n",
    "        tool_descriptions = toolbox.tools()\n",
    "        return tool_descriptions\n",
    "\n",
    "    def think(self, prompt):\n",
    "        \"\"\"\n",
    "        Runs the generate_text method on the model using the system prompt template and tool descriptions.\n",
    "\n",
    "        Parameters:\n",
    "        prompt (str): The user query to generate a response for.\n",
    "\n",
    "        Returns:\n",
    "        dict: The response from the model as a dictionary.\n",
    "        \"\"\"\n",
    "        tool_descriptions = self.prepare_tools()\n",
    "        agent_system_prompt = agent_system_prompt_template.format(tool_descriptions=tool_descriptions)\n",
    "\n",
    "        # Create an instance of the model service with the system prompt\n",
    "\n",
    "        if self.model_service == OllamaModel:\n",
    "            model_instance = self.model_service(\n",
    "                model=self.model_name,\n",
    "                system_prompt=agent_system_prompt,\n",
    "                temperature=0,\n",
    "                stop=self.stop\n",
    "            )\n",
    "        else:\n",
    "            model_instance = self.model_service(\n",
    "                model=self.model_name,\n",
    "                system_prompt=agent_system_prompt,\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "        # Generate and return the response dictionary\n",
    "        agent_response_dict = model_instance.generate_text(prompt)\n",
    "        return agent_response_dict\n",
    "\n",
    "    def work(self, prompt):\n",
    "        \"\"\"\n",
    "        Parses the dictionary returned from think and executes the appropriate tool.\n",
    "\n",
    "        Parameters:\n",
    "        prompt (str): The user query to generate a response for.\n",
    "\n",
    "        Returns:\n",
    "        The response from executing the appropriate tool or the tool_input if no matching tool is found.\n",
    "        \"\"\"\n",
    "        agent_response_dict = self.think(prompt)\n",
    "        tool_choice = agent_response_dict.get(\"tool_choice\")\n",
    "        tool_input = agent_response_dict.get(\"tool_input\")\n",
    "\n",
    "        for tool in self.tools:\n",
    "            if tool.__name__ == tool_choice:\n",
    "                response = tool(tool_input)\n",
    "\n",
    "                print(colored(response, 'cyan'))\n",
    "                return\n",
    "                # return tool(tool_input)\n",
    "\n",
    "        print(colored(tool_input, 'cyan'))\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUEST RESPONSE <Response [200]>\n",
      "\n",
      "\n",
      "Response from Ollama model: {'tool_choice': 'no tool', 'tool_input': \"You can ask me anything you'd like, such as mathematical calculations, wordplay, or any other type of question.\"}\n",
      "\u001b[36mYou can ask me anything you'd like, such as mathematical calculations, wordplay, or any other type of question.\u001b[0m\n",
      "REQUEST RESPONSE <Response [200]>\n",
      "\n",
      "\n",
      "Response from Ollama model: {'tool_choice': 'reverse_string', 'tool_input': 'my name is anuj sharma'}\n",
      "\u001b[36mThe reversed string is: amrahs juna si eman ym\n",
      "\n",
      ".Executed using the reverse_string function.\u001b[0m\n",
      "REQUEST RESPONSE <Response [200]>\n",
      "\n",
      "\n",
      "Response from Ollama model: {'tool_choice': 'basic_calculator', 'tool_input': '{\"num1\": 1, \"num2\": 2, \"operation\": \"add\"}'}\n",
      "\u001b[36m\n",
      "\n",
      "The answer is: 3.\n",
      "Calculated with basic_calculator.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(tools\u001b[38;5;241m=\u001b[39mtools, model_service\u001b[38;5;241m=\u001b[39mmodel_service, model_name\u001b[38;5;241m=\u001b[39mmodel_name, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAsk me anything: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/Projects/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/Projects/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    tools = [basic_calculator, reverse_string]\n",
    "\n",
    "    # Uncoment below to run with OpenAI\n",
    "    # model_service = OpenAIModel\n",
    "    # model_name = 'gpt-3.5-turbo'\n",
    "    # stop = None\n",
    "\n",
    "    # Uncomment below to run with Ollama\n",
    "    model_service = OllamaModel\n",
    "    model_name = \"llama3.1\"\n",
    "    stop = \"<|eot_id|>\"\n",
    "\n",
    "    agent = Agent(tools=tools, model_service=model_service, model_name=model_name, stop=stop)\n",
    "\n",
    "    while True:\n",
    "        prompt = input(\"Ask me anything: \")\n",
    "        if prompt.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        agent.work(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
